\section{Introdução}

Criado por volta dos anos 50, o nome Compilador se refere ao processo 
de composição de um programa através da reunião de várias rotinas de bibliotecas. 
O processo de tradução (de uma linguagem fonte para uma linguagem objeto), 
considerado hoje a função central de um compilador, era então conhecido como 
programação automática[Rangel, 1999]

Definido em [AHO, 1995], um compilador é um programa que lê outro programa 
escrito em uma linguagem --- a linguagem de origem --- e o traduz em um programa 
equivalente em outra linguagem --- a linguagem de destino. Como uma importante 
parte no processo de tradução, o compilador reporta ao seu usuário a presença 
de erros no programa origem.

Ao longo dos anos 50, os compiladores foram considerados programas notoriamente
difíceis de escrever. O primeiro compilador Fortran, por exemplo, consumiu
18-homens ano para implementar[Backus, 1957]. Desde então, foram descobertas
técnicas sistemáticas para o tratamento de muitas das mais importantes
tarefas desenvolvidas por um compilador.

A variedade de compiladores nos dias de hoje é muito grande. Existem inúmeras
linguagens fontes, as quais poderiam ser citadas em várias páginas deste trabalho. Isso
se deve principalmente ao fato de que com o aumento do uso dos computadores, aumentou
também, as necessidades de cada indivíduo, sendo estas específicas, exigindo por sua vez
linguagens de programação diferentes. Este processo --- juntamente com
a evolução da tecnologia de desenvolvimento de compiladores --- levou à criação de várias
técnicas diferentes para a construção de um compilador, ou seja, passou a existir
diferentes maneiras de se implementar um compilador. No entanto, a despeito dessa
aparente complexidade, as tarefas básicas que qualquer compilador precisa realizar
são essencialmente as mesmas.

A grande maioria dos compiladores de hoje fazem uso da técnica chamada: \textit{tradução
dirigida pela sintaxe}. Nesta técnica as regras de contrução do programa fonte são utilizadas
para guiar todo o processo de compilação.
Algumas das técnicas mais antigas utilizadas na contrução dos primeiros compiladores (da
linguagem Fortran) pode ser obtido em [Rosen, 1967].

\section{Modelo de Compilação de Análise e Síntese}

Ainda segundo [Rangel, 1999], existem duas tarefas triviais a serem executadas 
por um compilador nesse processo de tradução:

\begin{itemize}
 \item \textit{análise}, em que o texto de entrada (na linguagem fonte) é 
examinado, verificado e compreendido
 \item \textit{síntese}, ou \textit{geração de código}, em que o texto de saída 
(na linguagem objeto) é gerado, de forma a corresponder ao texto de entrada.
\end{itemize}

Em [Aho, 1995], \textit{análise} é colocada como uma tarefa que divide o programa
fonte nas partes constituintes e cria uma representação intermediária do mesmo. E
\textit{síntese} constrói o programa alvo desejado, a partir da representação intermediária.

Geralmente, pensamos nessas tarefas como fases que ocorram durante o processo de
 compilação. No entanto, não se faz totalmente necessário que a análise de todo 
o programa seja realizada antes que o primeiro trecho de código objeto seja gerado. 
Ou seja, estas duas fases podem ser intercaladas. Por exemplo, o compilador pode 
analisar cada comando do programa de entrada e então gerar de imediato o código 
de saída correspondente ao respectivo comando. Ou ainda, o compilador pode esperar 
pelo fim da análise de cada bloco de comando --- ou unidade de rotina 
(rotina, procedimentos, funções) --- para então gerar o código correspondente ao bloco.
Para aproveitar melhor a memória durante a execução, compiladores costumavam ser divididos
em várias etapas, executados em sequência. Cada etapa constitui uma parte do processo de
tradução, transformando assim o código fonte em alguma estrutura intermediária adequada,
cada vez mais próxima do código objeto final.

É natural que a análise retorne como resultado uma representação do programa fonte que
contenha informação necessária para a geração do programa objeto que o corresponda.
Quase sempre, essa representação (conhecida como \textit{representação intermediária}[Rangel, 1999])
tem como complemento tabelas que contêm informações adicionais sobre o programa fonte.
Pode ter casos em que a representação intermediária toma a forma de um programa em uma
\textit{linguagem intermediária}, deixando assim mais fácil a tradução para a linguagem
objeto desejada.

Não importando a maneira pela qual se toma a representação intermediária, ela tem de conter
necessariamente toda a informação para a geração do código objeto. Uma das características da
representação intermediária é que as estruturas de dados implementadas devem dar garantia de 
acesso eficiente as informações.

Imagem - Rangel[1999]

Segundo [Rangel, 1999], uma das formas mais comuns de tabela utilizada nessa representação
intermediária é a \textit{tabela de símbolos}, em que se guarda para cada identificador(\textit{símbolo})
usado no programa as informações correspondentes.

Há também um modelo possível em [Ullman, 1977], o qual se faz a separação total entre o \textit{front-end},
encarregado da fase de análise, e o \textit{back-end}, encarregado pela geração de código.
Com isso tem-se que:

\begin{itemize}
 \item front-end e back-end se comunicam apenas da representação intermediária;
 \item o front-end depende exclusivamente da linguagem fonte
 \item o back-end depende exclusivamente da linguagem objeto.
\end{itemize}

Essa idéia tem como objetivo simplificar a implementação de diferentes linguagens de programação para
diferentes máquinas. Basta-se então escrever um front-end para cada linguagem e um back-end para cada
máquina. Ou seja, se deseja implementar \textit{x} linguagens para \textit{y} máquinas, precisa-se fazer
\textit{x} front-ends e \textit{y} back-ends. Este esquema se torna mais fácil de aplicar quando há
semelhança entre as máquinas e o mesmo acontece com as linguagens.

\section{Análise}

É normal associar a \textit{sintaxe} a idéia de forma, em oposição a \textit{semântica} que se
associa a significado, conteúdo. Tem-se então que a sintaxe de uma linguagem de programação
deve descrever todos os aspectos relativos à forma de construção de programas corretos na linguagem,
enquanto a semântica deve descrever o que acontece quando o programa é executado. Portanto, toda
análise está relacionada com sintaxe, e a semântica deveria corresponder apenas à geração de código,
que deve preservar o significado do programa fonte, contruindo um programa objeto com o mesmo
significado[Rangel, 1999]

É necessário ressaltar uma diferença existente entre a teoria e a prática. Quando falamos em teoria,
somente os programas corretos pertencem à linguagem, não havendo interesse nos programas incorretos.
O fato é que um programa ou é da linguagem (está correto) ou não é da linguagem (está incorreto).
No entanto, em se tratando de prática, no momento em que decide-se que um programa está incorreto,
um bom compilador deverá ser capaz de avisar sobre tal erro e de alguma forma, ajudar o usuário
a corrigí-lo. Se faz necessário que o tratamento de erros inclua mensagens informativas e
uma recuperação, para que a análise possa continuar e assim outros erros sejam sinalizados.

Em [AHO, 1995] vemos que a análise se constitui em 3 fases:

\begin{itemize}
 \item \textit{Análise Linear - Análise Léxica}, na qual um fluxo de caracteres constituindo um programa é lido
da esquerda para a direita e agrupado em \textit{tokens}, que são sequências de caracteres tendo
um significado coletivo.
 \item \textit{Análise Hierárquica - Análise Sintática}, na qual os caracteres ou \textit{tokens} são agrupados
hierarquicamente em coleções aninhadas com significado coletivo.
 \item \textit{Análise Semântica}, na qual certas verificações são realizadas a fim de se
assegurar que os componentes de um programa se combinam de forma significativa.
\end{itemize}

Sabe-se da possibilidade de total representação da sintaxe de uma linguagem de programação
através de uma gramática sensível ao contexto [Wijngaarden, 1969]. No entanto, não
há algoritmos práticos para tratar estas gramáticas, fazendo com que haja preferência
em usar gramáticas livres de contexto. Sendo assim, fica claro que a separação entre
análise sintática e análise semântica é dependente da implementação.

Sendo assim, a análise léxica tem como finalidade separar e identificar os elementos
componentes do programa fonte, o qual estes geralmente, são especificados através de
expressões regulares. A análise sintática deve reconhecer a estrutura global do programa,
descrita através de gramáticas livre de contexto. A análise semântica se encarrega da
verificação das regras restantes. Essas regras tratam quase sempre da verificação de
que os objetos são usados no programa da maneira prevista em suas declarações, por exemplo
verificando que não há erros de tipos [Rangel, 1999].

Não há ainda um modelo matemático que se adeque inteiramente na função de descrever o que deve
ser verificado durante a análise semântica, ao contrário do que ocorre nas outras duas fases. No
entanto, alguns mecanismos, como gramática de atributos, tem sido utilizados com sucesso no processo
de simplificação da construção de analisadores semânticos.

\subsection{Análise Léxica}

Segundo [AHO, 1995], o analisador léxico é a primeira fase de um compilador. Sua tarefa principal é
a de ler os caracteres de entrada e produzir uma sequência de \textit{tokens} que o \textit{parser}
utiliza para a análise sintática. Essa interação é comumente implementada fazendo com que o analisador
léxico seja uma subrotina do \textit{parser}. Ao receber do \textit{parser} um comando 'obter o próximo
\textit{token}', o analisador léxico lê os caracteres de entrada até que possa identificar o próximo
\textit{token}.

Já em [Ullman, 1977], vemos que a análise léxica é responsável por separar e identificar os elementos componentes
do código fonte. A análise léxica também elimina os elementos considerados 'decorativos' ou mesmo desnecessários
para este processo, tais como espaços em branco, marcas de formatação de texto e comentários.

Em [Rangel, 1995] temos o seguinte exemplo em Pascal:

\begin{lstlisting}
 if x > 0 then        {x e' positivo}
     modx := x
 else                 {x e' negativo}
     modx := (-x)
\end{lstlisting}

Após a análise léxica, a sequência de \textit{tokens} identificadas é:

\begin{center}
\begin{tabular}{cc}
\hline
\textbf{Tipo do \textit{token}} & \textbf{Valor do \textit{token}}\\
\hline
palavra reservada if & if\\
\hline
identificador x & x\\
\hline
operador maior & >\\
\hline
literal numérico & 0\\
\hline
palavra reservada then & then\\
\hline
identificador & modx\\
\hline
operador de atribuição & :=\\
\hline
identificador & x\\
\hline
palavra reservada else & else\\
\hline
identificador & modx\\
\hline
operador de atribuição & :=\\
\hline
delimitador abre parêntese & (\\
\hline
operador menos unário & -\\
\hline
identificador & x\\
\hline
delimitador fecha parenteses & )\\
\hline
\end{tabular}
\end{center}

Normalmente os tipos dos \textit{tokens}(na primeira coluna) são representados por valores de um tipo de enumeração
ou por códigos numéricos apropriados.

O que vemos na grande maioria das vezes é que a implementação de um analisador léxico é baseada em um autômato
finito capaz de reconhecer as diversas construções.

\subsection{Análise Sintática}

As linguagens de programação possuem regras que descrevem a estrutura sintática dos programas bem-formados.
Desta maneira, a partir da sequência de \textit{tokens} gerada pelo analisador léxico, o analisador sintático
realiza o seu trabalho, verificando se esta sequência pode ser gerada pela gramática da linguagem-fonte.

A análise sintática envolve o agrupamento dos \textit{tokens} do programa fonte em frases gramaticais, que são
usadas pelo compilador, a fim de sintetizar a saída. Usualmente, as frases gramaticais do programa fonte são
representadas por uma árvore gramatical [AHO, 1995].

Já em [Rangel, 1999] temos que a análise sintática deve reconhecer a estrutura global do programa, por exemplo,
verificando que programas, comandos, declarações, expressões, etc têm as regras de composição respeitadas.

Vejamos o exemplo:

\begin{lstlisting}
 se x > 0 entao
     modx = x
 senao
     modx = -x
 fimsenao
\end{lstlisting}

Caberia a análise sintática reconhecer a estrutura deste trecho, reconhecendo de que se trata de um \textit{<comando>}
, que no caso é um \textit{<comando-se>}, composto pela palavra reserva \textit{se}, seguido de uma \textit{<expressão},
seguida também de uma palavra reservada \textit{entao}, e assim por diante. 

Ainda segundo [Rangel, 1999], quase universalmente, a sintaxe das linguagens de programação é descrita por gramáticas
livres de contexto, em uma notação chamada BNF (\textit{Forma de Backus-Naur} ou ainda \textit{Forma Normal de Backus},
ou em alguma variante ou extensão dessa notação. Essa notação foi introduzida por volta de 1960, para a descrição da
linguagem Algol [Naur, 1963].

Existem três tipos gerais de analisadores sintáticos. Os métodos universais de análise sintática, tais como o algoritmo
de \textit{Cocke-Younger-Casami} e o de \textit{Earley}, podem tratar qualquer gramática. Esses métodos, entretanto, são
muito ineficientes para se usar num compilador de produção. Os métodos mais comumentes usados nos compiladores
são classificados como \textit{top-down} ou \textit{bottom-up}. Como indicado por seus nomes, os analisadores
sintáticos \textit{top-down} constroem árvores do topo(raiz) para o fundo(folhas), enquanto que os \textit{bottom-up}
começam pelas folhas e trabalham árvore acima até a raiz. Em ambos os casos, a entrada é varrida da esquerda para
a direita, um símbolo de cada vez [AHO, 1995]

Os métodos de análise sintática mais eficientes, tanto \textit{top-down} quanto \textit{bottom-up}, trabalham
somente em determinadas subclasses de gramáticas, mas várias dessas subclasses, como as das grámaticas LL e LR, são
suficientemente expressivas para descrever a maioria das construções sintáticas das linguagens de programação.
Os analisadores implementados manualmente trabalham frequentemente com gramáticas LL [AHO, 1995]



